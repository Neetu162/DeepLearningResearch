{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNjDQjv01APa39edw33idD7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Neetu162/DeepLearningResearch/blob/master/KTFBinClass.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rLHyWugY7QeV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def vectorize(good_path, mal_path, adverse):\n",
        "    good_path = good_path\n",
        "    mal_path = mal_path\n",
        "\n",
        "    # read files\n",
        "    with open(good_path) as f:\n",
        "        gdprm = f.readlines()\n",
        "    with open(mal_path) as f:\n",
        "        mlprm = f.readlines()\n",
        "\n",
        "    # Concatenate good and mal samples\n",
        "    perms = gdprm + mlprm\n",
        "\n",
        "    # append the labels\n",
        "    # good is labeled 0\n",
        "    # malware is labeled 1\n",
        "    labels = np.array([])\n",
        "    for x in gdprm:\n",
        "        labels = np.append(labels, 0)\n",
        "    for x in mlprm:\n",
        "        labels = np.append(labels, 1)\n",
        "\n",
        "    # Define the sklearn vectorizer\n",
        "    count_vect = CountVectorizer(input=u'content', analyzer=u'word',\n",
        "                                 token_pattern='(\\\\b(:?uses-|optional-)?permission:\\s[^\\s]*)')\n",
        "    #time0 = timeit.default_timer()\n",
        "\n",
        "    # vectorize input\n",
        "    features = count_vect.fit_transform(perms)\n",
        "\n",
        "    # convert to dense matrix\n",
        "    features = features.todense()\n",
        "    features = np.array(features)\n",
        "\n",
        "    # This is in the case of adversarial learning\n",
        "    # Some of the labels will be wrong on purpose\n",
        "    if adverse:\n",
        "        print(\"Adversarial Learning\")\n",
        "        # keep track of how many of each were changed\n",
        "        count1 = 0\n",
        "        count2 = 0\n",
        "\n",
        "        gdprmsize = np.size(gdprm, 0)\n",
        "        mlprmszie = np.size(mlprm, 0)\n",
        "\n",
        "        # change 10% of the good labels\n",
        "        for i in range(0, gdprmsize // 10):\n",
        "            if labels[i] == 0:\n",
        "                count1 += 1\n",
        "                labels[i] = 1\n",
        "        print(\"Good Permissions Changed: %d\" % count1)\n",
        "\n",
        "        # change 10% of the malware labels\n",
        "        for i in range(gdprmsize, gdprmsize + mlprmszie // 10):\n",
        "            if labels[i] == 1:\n",
        "                count2 += 1\n",
        "                labels[i] = 0\n",
        "        print(\"Malware Permissions Changed: %d\" % count2)\n",
        "\n",
        "        total = count1 + count2\n",
        "        print(\"Total Permissions Changed: %d\" % total)\n",
        "\n",
        "    print(\"Done Vectorizing Data\")\n",
        "    return features, labels\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v4F3RSj472JX",
        "colab_type": "text"
      },
      "source": [
        "full_run method"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0d5AQSN-78HD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Method for a standard test -- Not Grid Search\n",
        "def full_run(modelName, features, labels, train_ratio, args):\n",
        "    # Get Vars from input args\n",
        "    print(\"inside full_run method \")\n",
        "    epochs = args[\"epochs\"]\n",
        "    batch_size = args[\"batch_size\"]\n",
        "    neurons = args[\"neurons\"]\n",
        "    optimizer = args[\"optimizer\"][0]\n",
        "    weight_constraint = args[\"weight_constraint\"]\n",
        "    dropout_rate = args[\"dropout\"]/100\n",
        "    percent = float(train_ratio) / 100\n",
        "    splits = args[\"splits\"]\n",
        "    print(\"variables assigned\")\n",
        "    #model_params = dict(batch_size=batch_size, epochs=epochs, neurons=neurons, optimizer=optimizer,\n",
        "    #                    weight_constraint=weight_constraint, dropout_rate=dropout_rate)\n",
        "\n",
        "    fit_params = dict(batch_size=batch_size, epochs=epochs)\n",
        "    scoring = ['accuracy', 'precision', 'recall', 'f1']\n",
        "\n",
        "    # Define and Build the Model based on input modelName\n",
        "    if modelName == \"oneLayer\":\n",
        "        model = KerasClassifier(build_fn=create_one_layer, batch_size=batch_size, epochs=epochs, neurons=neurons,\n",
        "                                optimizer=optimizer, weight_constraint=weight_constraint, dropout_rate=dropout_rate,\n",
        "                                verbose=2)\n",
        "    elif modelName == \"binaryDecrease\":\n",
        "        model = KerasClassifier(build_fn=create_binaryDecrease, batch_size=batch_size, epochs=epochs, neurons=neurons,\n",
        "                                optimizer=optimizer, weight_constraint=weight_constraint, dropout_rate=dropout_rate,\n",
        "                                verbose=2)\n",
        "    elif modelName == \"fourSame\":\n",
        "        model = KerasClassifier(build_fn=create_fourSameLayer, batch_size=batch_size, epochs=epochs, neurons=neurons,\n",
        "                                optimizer=optimizer, weight_constraint=weight_constraint, dropout_rate=dropout_rate,\n",
        "                                verbose=2)\n",
        "    elif modelName == \"fourDecr\":\n",
        "        print(\"fourDecr start\")\n",
        "        model = KerasClassifier(build_fn=create_fourDecrLayer, batch_size=batch_size, epochs=epochs, neurons=neurons,\n",
        "                                optimizer=optimizer, weight_constraint=weight_constraint, dropout_rate=dropout_rate,\n",
        "                                verbose=1)\n",
        "        print(\"fourDecr end\"]\n",
        "    # Shuffle split Definition for Cross Validation\n",
        "    print(\"sss start\")\n",
        "    sss = StratifiedShuffleSplit(n_splits=splits, test_size=percent, random_state=0)\n",
        "    print(\"sss end\")\n",
        "    # Running the model with Cross Validation\n",
        "    print(\"cv_result start\")\n",
        "    cv_result = cross_validate(model, features, labels, cv=sss, fit_params=fit_params, return_train_score=True,\n",
        "                               scoring=scoring, verbose=100)\n",
        "    print(\"cv_result end\")\n",
        "    # Determine date for creating a file later\n",
        "    # This helps to keep track of tests and prevents overwriting of results\n",
        "    d = datetime.datetime.today()\n",
        "    month = str( '%02d' % d.month)\n",
        "    day = str('%02d' % d.day)\n",
        "    hour = str('%02d' % d.hour)\n",
        "    min = str('%02d' % d.minute)\n",
        "\n",
        "    # saving the result of testing to a Pandas Dataframe\n",
        "    df = pandas.DataFrame(cv_result)\n",
        "    print(\"Writing to file ...\")\n",
        "    # Write the results out to a file\n",
        "    try:\n",
        "        path1 = '/home/osboxes/DeepLearningResearch/Classification/results/' + modelName + month + day + hour + min + '.csv'\n",
        "        file1 = open(path1, \"a+\")\n",
        "    except:\n",
        "        path1 = \"results\" + modelName + month + day + hour + min + \".csv\"\n",
        "        file1 = open(path1, \"a+\")\n",
        "    df.to_csv(file1, index=True)\n",
        "    file1.close()\n",
        "\n",
        "    return 0\n",
        "\n",
        "\n",
        "# Grid Search Method\n",
        "def grid_search(modelName, features, labels, train_ratio, args):\n",
        "\n",
        "    # Get Vars from input args\n",
        "    splits = args[\"splits\"]\n",
        "    percent = float(train_ratio) / 100\n",
        "    epochs = args[\"epochs\"]\n",
        "    batch_size = args[\"batch_size\"]\n",
        "    neurons = args[\"neurons\"]\n",
        "    optimizer = args[\"optimizer\"]\n",
        "    weight_constraint = args[\"weight_constraint\"]\n",
        "    dropout_rate = args[\"dropout\"]\n",
        "\n",
        "    # Define the grid based on params\n",
        "    paramGrid = dict(epochs=epochs, batch_size=batch_size, optimizer=optimizer,\n",
        "                     dropout_rate=dropout_rate, weight_constraint=weight_constraint,\n",
        "                     neurons=neurons)\n",
        "\n",
        "    # Model Definition based on input modelName\n",
        "    if modelName == \"oneLayer\":\n",
        "        model = KerasClassifier(build_fn=create_one_layer, verbose=0)\n",
        "    elif modelName == \"binaryDecrease\":\n",
        "        model = KerasClassifier(build_fn=create_binaryDecrease, verbose=0)\n",
        "    elif modelName == \"fourSame\":\n",
        "        model = KerasClassifier(build_fn=create_fourSameLayer, verbose=0)\n",
        "    elif modelName == \"fourDecr\":\n",
        "        model = KerasClassifier(build_fn=create_fourDecrLayer, verbose=0)\n",
        "\n",
        "    # Define Split and Grid Search Cross Validation\n",
        "    sss = StratifiedShuffleSplit(n_splits=splits, test_size=percent, random_state=0)\n",
        "    grid = GridSearchCV(estimator=model, param_grid=paramGrid, n_jobs=1, cv=sss, refit=True, verbose=2)\n",
        "\n",
        "    # Execute a grid search\n",
        "    grid_fit = grid.fit(features, labels)\n",
        "\n",
        "    # These are metrics that can be used later\n",
        "    means = grid_fit.cv_results_['mean_test_score']\n",
        "    stds = grid_fit.cv_results_['std_test_score']\n",
        "    params = grid_fit.cv_results_['params']\n",
        "\n",
        "    print(\"%s Best: %f using %s\" % (modelName, grid_fit.best_score_, grid_fit.best_params_))\n",
        "\n",
        "    # Determine date for creating a file later\n",
        "    # This helps to keep track of tests and prevents overwriting of results\n",
        "    d = datetime.datetime.today()\n",
        "    month = str( '%02d' % d.month)\n",
        "    day = str('%02d' % d.day)\n",
        "    hour = str('%02d' % d.hour)\n",
        "    min = str('%02d' % d.minute)\n",
        "\n",
        "    # Save results to  Pandas Dataframe\n",
        "    df = pandas.DataFrame(grid_fit.cv_results_)\n",
        "\n",
        "    # Write the results out to a file\n",
        "    try:\n",
        "        path1 = '/home/lab309/pythonScripts/testResults/deep_results/gridSearch' + modelName + month + day + hour + min + '.csv'\n",
        "        file1 = open(path1, \"w+\")\n",
        "    except:\n",
        "        path1 = \"gridSearch\" + modelName + \".csv\"\n",
        "        file1 = open(path1, \"w+\")\n",
        "    df.to_csv(file1, index=True)\n",
        "    file1.close()\n",
        "\n",
        "    return 0\n",
        "\n",
        "\n",
        "# Command Line Parameters are define in this method\n",
        "def parse_arguments():\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument(\"-gp\", \"--good_path\", help=\"Good File Path\")\n",
        "    parser.add_argument(\"-mp\", \"--mal_path\", help=\"Malware File Path\")\n",
        "    parser.add_argument(\"-ad\", \"--adverse\", help=\"Turns on Adversarial Learning\")\n",
        "    parser.add_argument(\"-m\", \"--mode\", help=\"Choose mode: full, grid\")\n",
        "    parser.add_argument(\"-e\", \"--epochs\", help=\"Number of Epochs\", type=int, nargs=\"*\")\n",
        "    parser.add_argument(\"-tr\", \"--train_ratio\", nargs=\"*\", type=int,\n",
        "                        help=\"Set Test Ratios. Enter as a percent (20,40,60,80). Can be a list space delimited\")\n",
        "    parser.add_argument(\"-bs\", \"--batch_size\", nargs=\"*\", type=int,\n",
        "                        help=\"Batch size. Can be a list space delimited\")\n",
        "    parser.add_argument(\"-n\", \"--neurons\", nargs=\"*\", type=int,\n",
        "                        help=\"Number of Neurons. Can be a list space delimited\")\n",
        "    parser.add_argument(\"-o\", \"--optimizer\", nargs=\"*\",\n",
        "                        help=\"Optimizers. Can be a list space delimited\")\n",
        "    parser.add_argument(\"-w\", \"--weight_constraint\", nargs=\"*\", type=int,\n",
        "                        help=\"Weight Constraint. Can be a list space delimited\")\n",
        "    parser.add_argument(\"-d\", \"--dropout\", nargs=\"*\", type=int,\n",
        "                        help=\"Dropout. Enter as percent (10,20,30,40...). Can be a list space delimited.\")\n",
        "    parser.add_argument(\"-model\", \"--model\", help=\"Select which model to run: all, one_layer, four_decr, four_same\")\n",
        "    parser.add_argument(\"-s\", \"--splits\", help=\"Number of Splits for SSS\", type=int)\n",
        "\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    arguments = {}\n",
        "\n",
        "    if args.good_path:\n",
        "        good_path = args.good_path\n",
        "        arguments[\"good_path\"] = good_path\n",
        "    else:\n",
        "        print(\"Needs Good Path with -gp or --good_path\")\n",
        "        sys.exit()\n",
        "\n",
        "    if args.mal_path:\n",
        "        mal_path = args.mal_path\n",
        "        arguments[\"mal_path\"] = mal_path\n",
        "    else:\n",
        "        print(\"Needs Malware Path with -mp or --mal_path\")\n",
        "        sys.exit()\n",
        "\n",
        "    if args.adverse:\n",
        "        adverse = True\n",
        "    else:\n",
        "        adverse = False\n",
        "    arguments[\"adverse\"] = adverse\n",
        "\n",
        "    if args.mode == \"grid\":\n",
        "        mode = \"grid\"\n",
        "        print(\"Mode is %s\" % mode)\n",
        "    else:\n",
        "        mode = \"full\"\n",
        "        print(\"Mode is %s\" % mode)\n",
        "    arguments[\"mode\"] = mode\n",
        "\n",
        "    if args.model == \"all\":\n",
        "        model = [\"oneLayer\", \"fourDecr\", \"fourSame\"]\n",
        "    elif args.model in [\"oneLayer\", \"fourDecr\", \"fourSame\"]:\n",
        "        model = [args.model]\n",
        "    else:\n",
        "        print(\"Defaulting to All models\")\n",
        "        model = [\"oneLayer\", \"fourDecr\", \"fourSame\"]\n",
        "    arguments[\"model\"] = model\n",
        "\n",
        "    if args.epochs:\n",
        "        epochs = args.epochs\n",
        "    else:\n",
        "        print(\"Defaulting to 16 epochs\")\n",
        "        epochs = 16\n",
        "    arguments[\"epochs\"] = epochs\n",
        "    if args.train_ratio:\n",
        "        train_ratio = args.train_ratio\n",
        "    else:\n",
        "        print(\"Defaulting to testing all ratios\")\n",
        "        train_ratio = [20, 40, 60, 80]\n",
        "    arguments[\"train_ratio\"] = train_ratio\n",
        "\n",
        "    if args.batch_size:\n",
        "        batch_size = args.batch_size\n",
        "    else:\n",
        "        print(\"Defaulting to Batch Size 10\")\n",
        "        batch_size = 10\n",
        "    arguments[\"batch_size\"] = batch_size\n",
        "\n",
        "    if args.neurons:\n",
        "        neurons = args.neurons\n",
        "    else:\n",
        "        print(\"Defaulting to 45 Neurons\")\n",
        "        neurons = 45\n",
        "    arguments[\"neurons\"] = neurons\n",
        "\n",
        "    if args.optimizer:\n",
        "        optimizer = args.optimizer\n",
        "    else:\n",
        "        print(\"Defaulting to NADAM Optimizer\")\n",
        "        optimizer = \"Nadam\"\n",
        "    arguments[\"optimizer\"] = optimizer\n",
        "\n",
        "    if args.weight_constraint:\n",
        "        weight_constraint = args.weight_constraint\n",
        "    else:\n",
        "        print(\"Defaulting to weight constraint 5\")\n",
        "        weight_constraint = 5\n",
        "    arguments[\"weight_constraint\"] = weight_constraint\n",
        "\n",
        "    if args.dropout:\n",
        "        dropout = args.dropout\n",
        "    else:\n",
        "        print(\"Defaulting to dropout of 10%\")\n",
        "        dropout = 10\n",
        "    arguments[\"dropout\"] = dropout\n",
        "\n",
        "    if args.splits:\n",
        "        splits = args.splits\n",
        "    else:\n",
        "        print(\"Defaulting to 1 SSS Split\")\n",
        "        splits = 1\n",
        "    arguments[\"splits\"] = splits\n",
        "\n",
        "    return arguments\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}