{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1kzRQzBf4LWfLZy6Hiotr1PzF7M30dNpE",
      "authorship_tag": "ABX9TyNUdsPhSj206sCeotixQmpz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Neetu162/DeepLearningResearch/blob/master/Run1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rLHyWugY7QeV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "c2e46242-4047-449f-d961-9081d0a391eb"
      },
      "source": [
        "import numpy as np\n",
        "import pandas\n",
        "import sys\n",
        "import argparse\n",
        "import datetime\n",
        "\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "from sklearn.model_selection import StratifiedShuffleSplit, cross_validate, GridSearchCV\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from keras.models import Sequential, Model\n",
        "from keras.constraints import maxnorm\n",
        "from keras.layers import Dense, Dropout, Input, concatenate\n",
        "from keras.optimizers import Nadam\n",
        "\n",
        "def create_one_layer(data_width, neurons=25, optimizer='adam', dropout_rate=0.0, weight_constraint=0):\n",
        "    #baseline Model\n",
        "    model = Sequential()\n",
        "    #The first param in Dense is the number of neurons in the first hidden layer\n",
        "    #model.add(Dense(neurons, input_dim=22300, kernel_initializer='normal', activation='relu',kernel_constraint=maxnorm(weight_constraint) ))\n",
        "    model.add(Dense(neurons, input_dim=data_width, kernel_initializer='normal', activation='relu'))\n",
        "    model.add(Dropout(dropout_rate))\n",
        "    model.add(Dense(1, kernel_initializer='normal', activation='sigmoid'))\n",
        "    # Compile model\n",
        "    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "\n",
        "def create_fourSameLayer(neurons=25, optimizer='adam', dropout_rate=0.0, weight_constraint=0):\n",
        "    #baseline Model\n",
        "    model = Sequential()\n",
        "    #The first param in Dense is the number of neurons in the first hidden layer\n",
        "    model.add(Dense(neurons, input_dim=22300, kernel_initializer='normal', activation='relu', kernel_constraint=maxnorm(weight_constraint)))\n",
        "    model.add(Dropout(dropout_rate))\n",
        "    model.add(Dense(neurons, kernel_initializer='normal', activation='relu', kernel_constraint=maxnorm(weight_constraint)))\n",
        "    model.add(Dropout(dropout_rate))\n",
        "    model.add(Dense(neurons, kernel_initializer='normal', activation='relu', kernel_constraint=maxnorm(weight_constraint)))\n",
        "    model.add(Dropout(dropout_rate))\n",
        "    model.add(Dense(neurons, kernel_initializer='normal', activation='relu', kernel_constraint=maxnorm(weight_constraint)))\n",
        "    model.add(Dropout(dropout_rate))\n",
        "    model.add(Dense(1, kernel_initializer='normal', activation='sigmoid'))\n",
        "    # Compile model\n",
        "    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "\n",
        "def create_fourDecrLayer(neurons=25, optimizer='adam', dropout_rate=0.0, weight_constraint=0):\n",
        "    #baseline Model\n",
        "    model = Sequential()\n",
        "    n2 = neurons // 2 if neurons // 2 > 0 else 1\n",
        "    n3 = neurons // 3 if neurons // 3 > 0 else 1\n",
        "    n4 = neurons // 4 if neurons // 4 > 0 else 1\n",
        "    #The first param in Dense is the number of neurons in the first hidden layer\n",
        "    model.add(Dense(neurons, input_dim=22300, kernel_initializer='normal', activation='relu', kernel_constraint=maxnorm(weight_constraint)))\n",
        "    model.add(Dropout(dropout_rate))\n",
        "    model.add(Dense(n2, kernel_initializer='normal', activation='relu', kernel_constraint=maxnorm(weight_constraint)))\n",
        "    if n2 > 1 : model.add(Dropout(dropout_rate))\n",
        "    model.add(Dense(n3, kernel_initializer='normal', activation='relu', kernel_constraint=maxnorm(weight_constraint)))\n",
        "    if n3 > 1: model.add(Dropout(dropout_rate))\n",
        "    model.add(Dense(n4, kernel_initializer='normal', activation='relu', kernel_constraint=maxnorm(weight_constraint)))\n",
        "    if n4 > 1: model.add(Dropout(dropout_rate))\n",
        "    model.add(Dense(1, kernel_initializer='normal', activation='sigmoid'))\n",
        "    # Compile model\n",
        "    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "\n",
        "def create_binaryDecrease(neurons=25, optimizer='adam'):\n",
        "    model = Sequential()\n",
        "    #The first param in Dense is the number of neurons in the first hidden layer\n",
        "    model.add(Dense(neurons, input_dim=22300, kernel_initializer='normal', activation='relu'))\n",
        "    while (neurons/2 >=1):\n",
        "        model.add(Dense(neurons/2, kernel_initializer='normal', activation='relu'))\n",
        "        neurons/=2\n",
        "    model.add(Dense(1, kernel_initializer='normal', activation='sigmoid'))\n",
        "    # Compile model\n",
        "    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "def create_dualInputSimple(input_ratio, feat_width, perm_width, neurons=32, dropout_rate=0.1):\n",
        "    '''this simple model performs no additional analysis after concatenation'''\n",
        "    perm_input = Input(shape=(perm_width,), name='permissions_input')\n",
        "    x = Dense(neurons, activation='relu')(perm_input)\n",
        "    feat_input = Input(shape=(feat_width,), name='features_input')\n",
        "    y = Dense(int(neurons*input_ratio), activation='relu')(feat_input)\n",
        "    x = concatenate([x, y])\n",
        "    output = Dense(1, activation='sigmoid', name='output')(x)\n",
        "    model = Model(inputs=[perm_input, feat_input], outputs=output)\n",
        "    model.compile(loss='binary_crossentropy', optimizer='nadam', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "def create_dualInputLarge(input_ratio, feat_width, perm_width, neurons=32, dropout_rate=0.1):\n",
        "    '''this model performs additional analysis with layers after concatenation'''\n",
        "    perm_width=int(perm_width)\n",
        "    perm_input = Input(shape=(perm_width,), name='permissions_input')\n",
        "    x = Dense(neurons, activation='relu')(perm_input)\n",
        "    x = Dropout(dropout_rate)(x)\n",
        "    x = Dense(neurons, activation='relu')(x)\n",
        "    feat_input = Input(shape=(feat_width,), name='features_input')\n",
        "    y = Dense(int(neurons*input_ratio), activation='relu')(feat_input)\n",
        "    x = concatenate([x, y])\n",
        "    x = Dense(int((neurons+(neurons*input_ratio))/2), activation='relu')(x)\n",
        "    x = Dropout(dropout_rate)(x)\n",
        "    x = Dense(int((neurons+(neurons*input_ratio))/2), activation='relu')(x)\n",
        "    output = Dense(1, activation='sigmoid', name=\"output\")(x)\n",
        "    model = Model(inputs=[perm_input, feat_input], outputs=output)\n",
        "    model.compile(loss='binary_crossentropy', optimizer='nadam', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "def vectorize(good_path, mal_path, adverse):\n",
        "    good_path = good_path\n",
        "    mal_path = mal_path\n",
        "\n",
        "    # read files\n",
        "    with open(good_path) as f:\n",
        "        gdprm = f.readlines()\n",
        "    with open(mal_path) as f:\n",
        "        mlprm = f.readlines()\n",
        "\n",
        "    # Concatenate good and mal samples\n",
        "    perms = gdprm + mlprm\n",
        "\n",
        "    # append the labels\n",
        "    # good is labeled 0\n",
        "    # malware is labeled 1\n",
        "    labels = np.array([])\n",
        "    for x in gdprm:\n",
        "        labels = np.append(labels, 0)\n",
        "    for x in mlprm:\n",
        "        labels = np.append(labels, 1)\n",
        "\n",
        "    # Define the sklearn vectorizer\n",
        "    count_vect = CountVectorizer(input=u'content', analyzer=u'word',\n",
        "                                 token_pattern='(\\\\b(:?uses-|optional-)?permission:\\s[^\\s]*)')\n",
        "    #time0 = timeit.default_timer()\n",
        "\n",
        "    # vectorize input\n",
        "    features = count_vect.fit_transform(perms)\n",
        "\n",
        "    # convert to dense matrix\n",
        "    features = features.todense()\n",
        "    features = np.array(features)\n",
        "\n",
        "    # This is in the case of adversarial learning\n",
        "    # Some of the labels will be wrong on purpose\n",
        "    if adverse:\n",
        "        print(\"Adversarial Learning\")\n",
        "        # keep track of how many of each were changed\n",
        "        count1 = 0\n",
        "        count2 = 0\n",
        "\n",
        "        gdprmsize = np.size(gdprm, 0)\n",
        "        mlprmszie = np.size(mlprm, 0)\n",
        "\n",
        "        # change 10% of the good labels\n",
        "        for i in range(0, gdprmsize // 10):\n",
        "            if labels[i] == 0:\n",
        "                count1 += 1\n",
        "                labels[i] = 1\n",
        "        print(\"Good Permissions Changed: %d\" % count1)\n",
        "\n",
        "        # change 10% of the malware labels\n",
        "        for i in range(gdprmsize, gdprmsize + mlprmszie // 10):\n",
        "            if labels[i] == 1:\n",
        "                count2 += 1\n",
        "                labels[i] = 0\n",
        "        print(\"Malware Permissions Changed: %d\" % count2)\n",
        "\n",
        "        total = count1 + count2\n",
        "        print(\"Total Permissions Changed: %d\" % total)\n",
        "\n",
        "    print(\"Done Vectorizing Data\")\n",
        "    return features, labels\n",
        "\n",
        "# Method for a standard test -- Not Grid Search\n",
        "def full_run(modelName, features, labels, train_ratio, args):\n",
        "    # Get Vars from input args\n",
        "    print(\"inside full_run method \")\n",
        "    epochs = args[\"epochs\"]\n",
        "    batch_size = args[\"batch_size\"]\n",
        "    neurons = args[\"neurons\"]\n",
        "    optimizer = args[\"optimizer\"][0]\n",
        "    weight_constraint = args[\"weight_constraint\"]\n",
        "    dropout_rate = args[\"dropout\"]/100\n",
        "    percent = float(train_ratio) / 100\n",
        "    splits = args[\"splits\"]\n",
        "    print(\"variables assigned\")\n",
        "    #model_params = dict(batch_size=batch_size, epochs=epochs, neurons=neurons, optimizer=optimizer,\n",
        "    #                    weight_constraint=weight_constraint, dropout_rate=dropout_rate)\n",
        "\n",
        "    fit_params = dict(batch_size=batch_size, epochs=epochs)\n",
        "    scoring = ['accuracy', 'precision', 'recall', 'f1']\n",
        "\n",
        "    # Define and Build the Model based on input modelName\n",
        "    if modelName == \"oneLayer\":\n",
        "        model = KerasClassifier(build_fn=create_one_layer, batch_size=batch_size, epochs=epochs, neurons=neurons,\n",
        "                                optimizer=optimizer, weight_constraint=weight_constraint, dropout_rate=dropout_rate,\n",
        "                                verbose=2)\n",
        "    elif modelName == \"binaryDecrease\":\n",
        "        model = KerasClassifier(build_fn=create_binaryDecrease, batch_size=batch_size, epochs=epochs, neurons=neurons,\n",
        "                                optimizer=optimizer, weight_constraint=weight_constraint, dropout_rate=dropout_rate,\n",
        "                                verbose=2)\n",
        "    elif modelName == \"fourSame\":\n",
        "        model = KerasClassifier(build_fn=create_fourSameLayer, batch_size=batch_size, epochs=epochs, neurons=neurons,\n",
        "                                optimizer=optimizer, weight_constraint=weight_constraint, dropout_rate=dropout_rate,\n",
        "                                verbose=2)\n",
        "    elif modelName == \"fourDecr\":\n",
        "        print(\"fourDecr start\")\n",
        "        model = KerasClassifier(build_fn=create_fourDecrLayer, batch_size=batch_size, epochs=epochs, neurons=neurons,\n",
        "                                optimizer=optimizer, weight_constraint=weight_constraint, dropout_rate=dropout_rate,\n",
        "                                verbose=1)\n",
        "        print(\"fourDecr end\")\n",
        "    # Shuffle split Definition for Cross Validation\n",
        "    print(\"sss start\")\n",
        "    sss = StratifiedShuffleSplit(n_splits=splits, test_size=percent, random_state=0)\n",
        "    print(\"sss end\")\n",
        "    # Running the model with Cross Validation\n",
        "    print(\"cv_result start\")\n",
        "    cv_result = cross_validate(model, features, labels, cv=sss, fit_params=fit_params, return_train_score=True,\n",
        "                               scoring=scoring, verbose=100)\n",
        "    print(\"cv_result end\")\n",
        "    # Determine date for creating a file later\n",
        "    # This helps to keep track of tests and prevents overwriting of results\n",
        "    d = datetime.datetime.today()\n",
        "    month = str( '%02d' % d.month)\n",
        "    day = str('%02d' % d.day)\n",
        "    hour = str('%02d' % d.hour)\n",
        "    min = str('%02d' % d.minute)\n",
        "\n",
        "    # saving the result of testing to a Pandas Dataframe\n",
        "    df = pandas.DataFrame(cv_result)\n",
        "    print(\"Writing to file ...\")\n",
        "    # Write the results out to a file\n",
        "    try:\n",
        "        path1 = '/home/osboxes/DeepLearningResearch/Classification/results/' + modelName + month + day + hour + min + '.csv'\n",
        "        file1 = open(path1, \"a+\")\n",
        "    except:\n",
        "        path1 = \"results\" + modelName + month + day + hour + min + \".csv\"\n",
        "        file1 = open(path1, \"a+\")\n",
        "    df.to_csv(file1, index=True)\n",
        "    file1.close()\n",
        "\n",
        "    return 0\n",
        "\n",
        "\n",
        "# Grid Search Method\n",
        "def grid_search(modelName, features, labels, train_ratio, args):\n",
        "\n",
        "    # Get Vars from input args\n",
        "    splits = args[\"splits\"]\n",
        "    percent = float(train_ratio) / 100\n",
        "    epochs = args[\"epochs\"]\n",
        "    batch_size = args[\"batch_size\"]\n",
        "    neurons = args[\"neurons\"]\n",
        "    optimizer = args[\"optimizer\"]\n",
        "    weight_constraint = args[\"weight_constraint\"]\n",
        "    dropout_rate = args[\"dropout\"]\n",
        "\n",
        "    # Define the grid based on params\n",
        "    paramGrid = dict(epochs=epochs, batch_size=batch_size, optimizer=optimizer,\n",
        "                     dropout_rate=dropout_rate, weight_constraint=weight_constraint,\n",
        "                     neurons=neurons)\n",
        "\n",
        "    # Model Definition based on input modelName\n",
        "    if modelName == \"oneLayer\":\n",
        "        model = KerasClassifier(build_fn=create_one_layer, verbose=0)\n",
        "    elif modelName == \"binaryDecrease\":\n",
        "        model = KerasClassifier(build_fn=create_binaryDecrease, verbose=0)\n",
        "    elif modelName == \"fourSame\":\n",
        "        model = KerasClassifier(build_fn=create_fourSameLayer, verbose=0)\n",
        "    elif modelName == \"fourDecr\":\n",
        "        model = KerasClassifier(build_fn=create_fourDecrLayer, verbose=0)\n",
        "\n",
        "    # Define Split and Grid Search Cross Validation\n",
        "    sss = StratifiedShuffleSplit(n_splits=splits, test_size=percent, random_state=0)\n",
        "    grid = GridSearchCV(estimator=model, param_grid=paramGrid, n_jobs=1, cv=sss, refit=True, verbose=2)\n",
        "\n",
        "    # Execute a grid search\n",
        "    grid_fit = grid.fit(features, labels)\n",
        "\n",
        "    # These are metrics that can be used later\n",
        "    means = grid_fit.cv_results_['mean_test_score']\n",
        "    stds = grid_fit.cv_results_['std_test_score']\n",
        "    params = grid_fit.cv_results_['params']\n",
        "\n",
        "    print(\"%s Best: %f using %s\" % (modelName, grid_fit.best_score_, grid_fit.best_params_))\n",
        "\n",
        "    # Determine date for creating a file later\n",
        "    # This helps to keep track of tests and prevents overwriting of results\n",
        "    d = datetime.datetime.today()\n",
        "    month = str( '%02d' % d.month)\n",
        "    day = str('%02d' % d.day)\n",
        "    hour = str('%02d' % d.hour)\n",
        "    min = str('%02d' % d.minute)\n",
        "\n",
        "    # Save results to  Pandas Dataframe\n",
        "    df = pandas.DataFrame(grid_fit.cv_results_)\n",
        "\n",
        "    # Write the results out to a file\n",
        "    try:\n",
        "        path1 = '/content/drive/My Drive/data/gridSearch' + modelName + month + day + hour + min + '.csv'\n",
        "        file1 = open(path1, \"w+\")\n",
        "    except:\n",
        "        path1 = \"gridSearch\" + modelName + \".csv\"\n",
        "        file1 = open(path1, \"w+\")\n",
        "    df.to_csv(file1, index=True)\n",
        "    file1.close()\n",
        "\n",
        "    return 0\n",
        "\n",
        "# Command Line Parameters are define in this method\n",
        "def parse_arguments():\n",
        "    arguments = {}\n",
        "\n",
        "    arguments[\"good_path\"] = \"/content/drive/My Drive/data/goodPermissionsFinal.txt\"\n",
        "    arguments[\"mal_path\"] = \"/content/drive/My Drive/data/malwarePermissionsFinal.txt\"\n",
        "    adverse = False\n",
        "    arguments[\"adverse\"] = adverse\n",
        "    mode = \"full\"\n",
        "    arguments[\"mode\"] = mode\n",
        "    model = [\"fourDecr\"]\n",
        "    arguments[\"model\"] = model\n",
        "    epochs = 16\n",
        "    arguments[\"epochs\"] = epochs\n",
        "    train_ratio = [20, 40, 60, 80]\n",
        "    arguments[\"train_ratio\"] = train_ratio\n",
        "    batch_size = 10\n",
        "    arguments[\"batch_size\"] = batch_size\n",
        "    neurons = 45\n",
        "    arguments[\"neurons\"] = neurons\n",
        "    optimizer = \"Nadam\"\n",
        "    arguments[\"optimizer\"] = optimizer\n",
        "    weight_constraint = 5\n",
        "    arguments[\"weight_constraint\"] = weight_constraint\n",
        "    dropout = 10\n",
        "    arguments[\"dropout\"] = dropout\n",
        "    splits = 1\n",
        "    arguments[\"splits\"] = splits\n",
        "    return arguments\n",
        "\n",
        "args = parse_arguments()\n",
        "print(\"Arguments\" + str(args))\n",
        "# vec Vectorize the input\n",
        "features, labels = vectorize(args[\"good_path\"], args[\"mal_path\"], args[\"adverse\"])\n",
        "print(\"Completed the vectorize\")\n",
        "# Grid Search\n",
        "if args[\"mode\"] == \"grid\" :\n",
        "    # for all models  \n",
        "    for m in args[\"model\"] :\n",
        "        # For all ratios\n",
        "        for r in args[\"train_ratio\"] :\n",
        "            grid_search(m, features, labels, r, args)\n",
        "\n",
        "# Regular run -- Not Grid Search\n",
        "else :\n",
        "    # For all models\n",
        "    for m in args[\"model\"] :\n",
        "        # For all ratios\n",
        "        for r in args[\"train_ratio\"] :\n",
        "            full_run(m, features, labels, r, args)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Arguments{'good_path': '/content/drive/My Drive/data/goodPermissionsFinal.txt', 'mal_path': '/content/drive/My Drive/data/malwarePermissionsFinal.txt', 'adverse': False, 'mode': 'full', 'model': ['fourDecr'], 'epochs': 16, 'train_ratio': [20, 40, 60, 80], 'batch_size': 10, 'neurons': 45, 'optimizer': 'Nadam', 'weight_constraint': 5, 'dropout': 10, 'splits': 1}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v4F3RSj472JX",
        "colab_type": "text"
      },
      "source": [
        "full_run method"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QXKp2_2TAIG7",
        "colab_type": "text"
      },
      "source": [
        "Google drive path: /content/drive/My Drive/data"
      ]
    }
  ]
}